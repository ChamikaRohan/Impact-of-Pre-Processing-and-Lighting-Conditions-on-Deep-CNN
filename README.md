# Impact-of-Pre-processing-and-Lighting-Conditions-on--Deep-CNN

## Introdcution

This project aims to analyze how different light conditions and preprocessing techniques affect the image 
classification model's performance. This particular project is about Dog and Rabbit image classification. In 
this project 5 videos were captured for each object under 5 different lighting conditions such as: (i) Sunny, 
(ii) Cloudy, (iii) Object in shadow, (iv) Indoor artificial light, and (v) Indoor natural light. Then dataset was 
generated by decomposing each video for 300 frames thereby 1500 images per object were obtained. By 
exploring different preprocessing teqniques this project aims to increase the model performance under 
different lighting conditions

## Methodology

The methodology began with a detailed data collection process, where 20-second videos of dogs and rabbits were recorded under standardized lighting conditions. Each video was tagged by object type and lighting condition, ensuring continuity and consistency. Using a Python script, these videos were decomposed into 300 frames each, creating a substantial image dataset. The images were then renamed and processed to remove black strips through contour detection. This process involved converting images to grayscale, applying a binary mask, detecting contours, and cropping the images accordingly. Finally, all images were resized to 75x75 pixels to ensure uniformity across the dataset, resulting in a well-prepared collection of images ready for model development.

The experimental setup involved training and evaluating multiple model variants to classify dogs and rabbits under different lighting conditions. The base model employed transfer learning with the InceptionV3 architecture, incorporating global average pooling and dense layers for feature processing. To enhance performance, pre-processing techniques such as image sharpening, color correction, and data augmentation were applied. Two variants of the base model were developed: the first variant added additional convolutional layers for intricate feature extraction, while the second variant further included dropout layers to mitigate overfitting. Additionally, a pre-trained VGG16 model was fine-tuned from the 17th layer onwards, leveraging its robust architecture and transfer learning capabilities. For each model, the dataset was split into training and validation sets in a ratio of 80:20. Training procedures involved 10 epochs for all models, except for the VGG16 model, which was limited to 5 epochs to avoid overfitting. The models were evaluated using accuracy, precision, recall, F1-score, and the area under the receiver operating characteristic curve (AU-ROC).

## Results

The results demonstrated that the VGG16 model achieved the highest accuracy, scoring a perfect 1.0, indicating flawless classification of images. Variant 2 of the base model exhibited the highest precision (0.559), recall (0.593), and F1-score (0.576), suggesting it was the most effective in distinguishing classes and avoiding false positives. The base model, without pre-processing, showed the lowest accuracy (0.806), while the inclusion of pre-processing techniques improved its performance to 0.826. Variant 1, with additional convolutional layers, achieved an accuracy of 0.943, showing significant improvement over the base model. Overall, the enhanced models demonstrated superior performance, with the VGG16 model excelling in accuracy and Variant 2 proving to be the best balanced in terms of precision, recall, and F1-score.

